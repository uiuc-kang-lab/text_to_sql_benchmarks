**Task Overview:**
Given a table in Markdown format, your task is to complete the following three tasks:

## Task 1
Analyze the table's header and rows to briefly summarize its domain. The output domain should be surrounded by [START_DOMAIN] and [END_DOMAIN].

## Task 2
Based on the table, propose a complex, enterprise-level, real-world business scenario. The output scenario should be surrounded by [START_SCENARIO] and [END_SCENARIO].

## Task 3
Create a comprehensive database schema in json format for the proposed business scenario. The output database schema should be surrounded by [START_DATABASE_SCHEMA] and [END_DATABASE_SCHEMA].

The schema for each table should comprise the following seven key attributes:
- `table_name`: The identifier or name assigned to the table.
- `table_description`: A brief narrative explaining the purpose or content of the table.
- `column_names`: A list of all column names within the table. Note that some column names might be abbreviations or non-descriptive strings, reflecting potential annotation inconsistencies in real-world database scenarios.
- `column_types`: The data types assigned to each column, acknowledging that some types might be more complex (e.g., Date, Array, Struct) than basic data types.
- `column_descriptions`: Detailed explanations for each column, providing semantic clarity on the data they hold.
- `primary_key`: The column(s) designated as the primary key for the table, ensuring the uniqueness of each row.
- `sample_rows`: An inclusion of two sample rows of data for the table, offering a practical glimpse into the table's data structure and content.

Upon detailing all tables, establish and define the foreign key relationships within the database schema:
- `foreign_keys`: A specification of the relationships between tables, indicating the source table, the referenced table, and the columns that are involved in this relationship.

Here are the refined notes for Task 3:
1. **Database Schema Design**: Develop a detailed and comprehensive set of tables and columns to accurately represent the complexity, scalability, and real-world business requirements of an enterprise-level application.
2. **Table and Foreign Key Constraints**: Ensure that the specified number of tables are created, incorporating the appropriate number of foreign key constraints to maintain data integrity and relationships.
3. **Ultra-Wide Tables**: Incorporate ultra-wide tables in your schema design, featuring 15-50 columns, to accommodate complex data structures and business needs.

Here, we present two demonstrations for reference:
**Demonstration 1:**
Complete the tasks and generate a database schema containing 5 tables based on the following table:
[Markdown Table]
```md
|    |   No. | DocumentÂ Title                                               |     ID | Site                                       | Updated    |   Hits |
|---:|------:|:-------------------------------------------------------------|-------:|:-------------------------------------------|:-----------|-------:|
|  0 |   841 | Canvas, How to Change the Course Name                        | 120983 | University of Illinois Technology Services | 2022-08-31 |    821 |
|  1 |   842 | Contracts+ Using the Jaggaer Word App during Contract Review | 120334 | UI Training and Development Resources      | 2023-04-24 |    822 |
|  2 |   843 | UAFR - Searching for Parent/Child Items                      | 119255 | UI Training and Development Resources      | 2023-04-24 |    822 |
|  3 |   844 | Gies Classroom, 1029C BIF (MIL)                              | 116771 | UI Gies College of Business                | 2023-03-08 |    822 |
|  4 |   845 | UAFR - Using the Manage Inventory Page                       | 119244 | UI Training and Development Resources      | 2023-04-24 |    825 |
```

[START_DOMAIN]
University Document Management and Resource Tracking
[END_DOMAIN]

[START_SCENARIO]
An enterprise-level university system is implementing a Document Management and Analytics Platform to handle educational resources, training materials, and institutional guidelines. The platform should support various academic and administrative departments, allowing them to upload, manage, and track the usage of important documents, such as training guides, course syllabi, contracts, and instructional resources. Departments need robust analytics tools to monitor document usage across different sites and ensure that up-to-date information is easily accessible.

Each department should have the ability to categorize documents, assign authorship, and monitor user engagement. The platform should also feature granular reporting capabilities to highlight which resources are most used, help identify outdated materials, and track access across different organizational units.

The system must accommodate complex relationships between documents, such as parent/child relationships for versions of documents, shared ownership across departments, and detailed history for document updates and revisions. Additionally, advanced user roles and permissions are required to manage document access and editing rights.
[END_SCENARIO]

[START_DATABASE_SCHEMA]
{{
  "table_num": 5,
  "tables": [
    {{
      "table_name": "documents",
      "table_description": "Table to store details of each document uploaded to the platform.",
      "column_names": ["doc_id", "doc_title", "doc_version", "site_id", "upload_date", "last_updated", "hits", "author_id"],
      "column_types": ["INTEGER", "VARCHAR", "INTEGER", "INTEGER", "DATE", "DATE", "INTEGER", "INTEGER"],
      "column_descriptions": [
        "Unique identifier for each document",
        "Title of the document",
        "Version number of the document",
        "Reference to the site or department where the document is hosted",
        "Date the document was uploaded",
        "Date the document was last updated",
        "Number of views or hits the document has received",
        "ID of the author who uploaded the document"
      ],
      "primary_key": ["doc_id"],
      "sample_rows": [
        [120983, "Canvas, How to Change the Course Name", 1, 101, "2022-08-01", "2022-08-31", 821, 3001],
        [120334, "Contracts+ Using Jaggaer Word App", 1, 102, "2023-04-01", "2023-04-24", 822, 3002]
      ]
    }},
    {{
      "table_name": "sites",
      "table_description": "Details of the sites or departments hosting the documents.",
      "column_names": ["site_id", "site_name", "department", "contact_email"],
      "column_types": ["INTEGER", "VARCHAR", "VARCHAR", "VARCHAR"],
      "column_descriptions": [
        "Unique identifier for each site",
        "Name of the site or department",
        "Department under which the site operates",
        "Contact email for inquiries about the site"
      ],
      "primary_key": ["site_id"],
      "sample_rows": [
        [101, "University of Illinois Technology Services", "Technology Services", "techsupport@uillinois.edu"],
        [102, "UI Training and Development Resources", "Training and Development", "train@uillinois.edu"]
      ]
    }},
    {{
      "table_name": "authors",
      "table_description": "Information about the authors uploading the documents.",
      "column_names": ["author_id", "author_name", "email", "department"],
      "column_types": ["INTEGER", "VARCHAR", "VARCHAR", "VARCHAR"],
      "column_descriptions": [
        "Unique identifier for each author",
        "Full name of the author",
        "Email address of the author",
        "Department the author belongs to"
      ],
      "primary_key": ["author_id"],
      "sample_rows": [
        [3001, "John Doe", "jdoe@uillinois.edu", "Technology Services"],
        [3002, "Jane Smith", "jsmith@uillinois.edu", "Training and Development"]
      ]
    }},
    {{
      "table_name": "document_access",
      "table_description": "Tracks access to documents by users.",
      "column_names": ["access_id", "doc_id", "user_id", "access_date", "access_type"],
      "column_types": ["INTEGER", "INTEGER", "INTEGER", "DATE", "VARCHAR"],
      "column_descriptions": [
        "Unique identifier for each access event",
        "ID of the document being accessed",
        "ID of the user accessing the document",
        "Date when the document was accessed",
        "Type of access (e.g., view, download)"
      ],
      "primary_key": ["access_id"],
      "sample_rows": [
        [5001, 120983, 4001, "2023-05-01", "view"],
        [5002, 120334, 4002, "2023-05-02", "download"]
      ]
    }},
    {{
      "table_name": "users",
      "table_description": "Details of users accessing the documents.",
      "column_names": ["user_id", "user_name", "email", "role"],
      "column_types": ["INTEGER", "VARCHAR", "VARCHAR", "VARCHAR"],
      "column_descriptions": [
        "Unique identifier for each user",
        "Full name of the user",
        "Email address of the user",
        "Role of the user (e.g., student, staff, admin)"
      ],
      "primary_key": ["user_id"],
      "sample_rows": [
        [4001, "Alice Johnson", "alice.johnson@uillinois.edu", "student"],
        [4002, "Bob Williams", "bob.williams@uillinois.edu", "staff"]
      ]
    }}
  ]
  "foreign_keys": [
    {{
      "source_table": "documents",
      "column_in_source_table": "site_id",
      "referenced_table": "sites",
      "column_in_referenced_table": "site_id"
    }},
    {{
      "source_table": "documents",
      "column_in_source_table": "author_id",
      "referenced_table": "authors",
      "column_in_referenced_table": "author_id"
    }},
    {{
      "source_table": "document_access",
      "column_in_source_table": "doc_id",
      "referenced_table": "documents",
      "column_in_referenced_table": "doc_id"
    }},
    {{
      "source_table": "document_access",
      "column_in_source_table": "user_id",
      "referenced_table": "users",
      "column_in_referenced_table": "user_id"
    }}
  ]
}}
[END_DATABASE_SCHEMA]

**Demonstration 2:**
Complete the tasks and generate a database schema containing 10 tables based on the following table:
[Markdown Table]
```md
|    |   Dataset Number | Site                                                                                                              | Category         | Name                | Type         | Frequency   | Year     | Data     | Readme   |
|---:|-----------------:|:------------------------------------------------------------------------------------------------------------------|:-----------------|:--------------------|:-------------|:------------|:---------|:---------|:---------|
|  0 |              151 | West Branch, Iowa, United States (WBI) Air samples collected in glass flasks.                                     | Greenhouse Gases | Carbon Dioxide(CO2) | Surface PFP  | Discrete    | Multiple | Download | Readme   |
|  1 |              152 | Walnut Grove, California, United States (WGC) Air samples collected in glass flasks.                              | Greenhouse Gases | Carbon Dioxide(CO2) | Aircraft PFP | Discrete    | Multiple | Download | Readme   |
|  2 |              153 | Walnut Grove, California, United States (WGC) Air samples collected in glass flasks.                              | Greenhouse Gases | Carbon Dioxide(CO2) | Surface PFP  | Discrete    | Multiple | Download | Readme   |
|  3 |              154 | Weizmann Institute of Science at the Arava Institute, Ketura, Israel (WIS) Air samples collected in glass flasks. | Greenhouse Gases | Carbon Dioxide(CO2) | Flask        | Discrete    | Multiple | Download | Readme   |
|  4 |              155 | Moody, Texas, United States (WKT) Air samples collected in glass flasks.                                          | Greenhouse Gases | Carbon Dioxide(CO2) | Surface PFP  | Discrete    | Multiple | Download | Readme   |
```

[START_DOMAIN]
Environmental Data Collection and Analysis
[END_DOMAIN]

[START_SCENARIO]
An international environmental research organization is developing an enterprise-level data management system to collect, store, and analyze environmental data from various sites around the world. The system will handle data related to greenhouse gases, air quality, and other environmental metrics. The organization needs a robust platform that can manage multiple datasets, track data collection methods, and provide detailed metadata for each dataset.

The system must support multiple sites, each with its own unique data collection methods and frequencies. It should also allow for the categorization of data by type (e.g., greenhouse gases, air quality) and provide detailed documentation (readme files) for each dataset. The platform should enable researchers to access and analyze data efficiently, with advanced search and filtering capabilities. Additionally, the system must support versioning of datasets to track changes over time and ensure data integrity.

The organization also requires a comprehensive user management system to control access to sensitive environmental data, with roles and permissions tailored to researchers, data analysts, and administrators. The platform should facilitate collaboration among different research teams and provide reporting tools to monitor data usage and trends.
[END_SCENARIO]

[START_DATABASE_SCHEMA]
{{
  "table_num": 10,
  "tables": [
    {{
      "table_name": "datasets",
      "table_description": "Table to store details of each dataset collected from various sites.",
      "column_names": ["dataset_id", "site_id", "category", "name", "type", "frequency", "year", "data_file", "readme_file"],
      "column_types": ["INTEGER", "INTEGER", "VARCHAR", "VARCHAR", "VARCHAR", "VARCHAR", "VARCHAR", "VARCHAR", "VARCHAR"],
      "column_descriptions": [
        "Unique identifier for each dataset",
        "Reference to the site where the data was collected",
        "Category of the data (e.g., Greenhouse Gases)",
        "Name of the data (e.g., Carbon Dioxide(CO2))",
        "Type of data collection method (e.g., Surface PFP, Aircraft PFP)",
        "Frequency of data collection (e.g., Discrete, Continuous)",
        "Year(s) the data was collected",
        "File path to the data file",
        "File path to the readme file"
      ],
      "primary_key": ["dataset_id"],
      "sample_rows": [
        [151, 1001, "Greenhouse Gases", "Carbon Dioxide(CO2)", "Surface PFP", "Discrete", "Multiple", "data/151.csv", "readme/151.txt"],
        [152, 1002, "Greenhouse Gases", "Carbon Dioxide(CO2)", "Aircraft PFP", "Discrete", "Multiple", "data/152.csv", "readme/152.txt"]
      ]
    }},
    {{
      "table_name": "sites",
      "table_description": "Details of the sites where data is collected.",
      "column_names": ["site_id", "site_name", "location", "contact_email"],
      "column_types": ["INTEGER", "VARCHAR", "VARCHAR", "VARCHAR"],
      "column_descriptions": [
        "Unique identifier for each site",
        "Name of the site",
        "Location of the site (e.g., West Branch, Iowa, United States)",
        "Contact email for inquiries about the site"
      ],
      "primary_key": ["site_id"],
      "sample_rows": [
        [1001, "West Branch, Iowa, United States (WBI)", "West Branch, Iowa, United States", "wbi@example.com"],
        [1002, "Walnut Grove, California, United States (WGC)", "Walnut Grove, California, United States", "wgc@example.com"]
      ]
    }},
    {{
      "table_name": "categories",
      "table_description": "Categories of data collected (e.g., Greenhouse Gases, Air Quality).",
      "column_names": ["category_id", "category_name", "description"],
      "column_types": ["INTEGER", "VARCHAR", "VARCHAR"],
      "column_descriptions": [
        "Unique identifier for each category",
        "Name of the category",
        "Description of the category"
      ],
      "primary_key": ["category_id"],
      "sample_rows": [
        [1, "Greenhouse Gases", "Data related to greenhouse gases"],
        [2, "Air Quality", "Data related to air quality"]
      ]
    }},
    {{
      "table_name": "data_types",
      "table_description": "Types of data collection methods used.",
      "column_names": ["type_id", "type_name", "description"],
      "column_types": ["INTEGER", "VARCHAR", "VARCHAR"],
      "column_descriptions": [
        "Unique identifier for each data type",
        "Name of the data type (e.g., Surface PFP, Aircraft PFP)",
        "Description of the data type"
      ],
      "primary_key": ["type_id"],
      "sample_rows": [
        [1, "Surface PFP", "Surface Profiler"],
        [2, "Aircraft PFP", "Aircraft Profiler"]
      ]
    }},
    {{
      "table_name": "frequencies",
      "table_description": "Frequencies of data collection.",
      "column_names": ["frequency_id", "frequency_name", "description"],
      "column_types": ["INTEGER", "VARCHAR", "VARCHAR"],
      "column_descriptions": [
        "Unique identifier for each frequency",
        "Name of the frequency (e.g., Discrete, Continuous)",
        "Description of the frequency"
      ],
      "primary_key": ["frequency_id"],
      "sample_rows": [
        [1, "Discrete", "Data collected at specific intervals"],
        [2, "Continuous", "Data collected continuously"]
      ]
    }},
    {{
      "table_name": "years",
      "table_description": "Years during which data was collected.",
      "column_names": ["year_id", "year_name"],
      "column_types": ["INTEGER", "VARCHAR"],
      "column_descriptions": [
        "Unique identifier for each year",
        "Year(s) the data was collected"
      ],
      "primary_key": ["year_id"],
      "sample_rows": [
        [1, "Multiple"],
        [2, "2023"]
      ]
    }},
    {{
      "table_name": "data_files",
      "table_description": "Details of the data files associated with each dataset.",
      "column_names": ["file_id", "dataset_id", "file_path", "file_size", "upload_date"],
      "column_types": ["INTEGER", "INTEGER", "VARCHAR", "FLOAT", "DATE"],
      "column_descriptions": [
        "Unique identifier for each data file",
        "ID of the dataset the file belongs to",
        "File path to the data file",
        "Size of the data file in MB",
        "Date the file was uploaded"
      ],
      "primary_key": ["file_id"],
      "sample_rows": [
        [1, 151, "data/151.csv", 1.2, "2023-01-01"],
        [2, 152, "data/152.csv", 1.5, "2023-01-02"]
      ]
    }},
    {{
      "table_name": "readme_files",
      "table_description": "Details of the readme files associated with each dataset.",
      "column_names": ["readme_id", "dataset_id", "file_path", "file_size", "upload_date"],
      "column_types": ["INTEGER", "INTEGER", "VARCHAR", "FLOAT", "DATE"],
      "column_descriptions": [
        "Unique identifier for each readme file",
        "ID of the dataset the readme file belongs to",
        "File path to the readme file",
        "Size of the readme file in MB",
        "Date the file was uploaded"
      ],
      "primary_key": ["readme_id"],
      "sample_rows": [
        [1, 151, "readme/151.txt", 0.1, "2023-01-01"],
        [2, 152, "readme/152.txt", 0.1, "2023-01-02"]
      ]
    }},
    {{
      "table_name": "users",
      "table_description": "Details of users accessing the datasets.",
      "column_names": ["user_id", "user_name", "email", "role"],
      "column_types": ["INTEGER", "VARCHAR", "VARCHAR", "VARCHAR"],
      "column_descriptions": [
        "Unique identifier for each user",
        "Full name of the user",
        "Email address of the user",
        "Role of the user (e.g., researcher, data analyst, admin)"
      ],
      "primary_key": ["user_id"],
      "sample_rows": [
        [1, "Alice Johnson", "alice.johnson@example.com", "researcher"],
        [2, "Bob Williams", "bob.williams@example.com", "data analyst"]
      ]
    }},
    {{
      "table_name": "access_logs",
      "table_description": "Tracks access to datasets by users.",
      "column_names": ["access_id", "dataset_id", "user_id", "access_date", "access_type"],
      "column_types": ["INTEGER", "INTEGER", "INTEGER", "DATE", "VARCHAR"],
      "column_descriptions": [
        "Unique identifier for each access event",
        "ID of the dataset being accessed",
        "ID of the user accessing the dataset",
        "Date when the dataset was accessed",
        "Type of access (e.g., view, download)"
      ],
      "primary_key": ["access_id"],
      "sample_rows": [
        [1, 151, 1, "2023-05-01", "view"],
        [2, 152, 2, "2023-05-02", "download"]
      ]
    }}
  ],
  "foreign_keys": [
    {{
      "source_table": "datasets",
      "column_in_source_table": "site_id",
      "referenced_table": "sites",
      "column_in_referenced_table": "site_id"
    }},
    {{
      "source_table": "datasets",
      "column_in_source_table": "category",
      "referenced_table": "categories",
      "column_in_referenced_table": "category_id"
    }},
    {{
      "source_table": "datasets",
      "column_in_source_table": "type",
      "referenced_table": "data_types",
      "column_in_referenced_table": "type_id"
    }},
    {{
      "source_table": "datasets",
      "column_in_source_table": "frequency",
      "referenced_table": "frequencies",
      "column_in_referenced_table": "frequency_id"
    }},
    {{
      "source_table": "datasets",
      "column_in_source_table": "year",
      "referenced_table": "years",
      "column_in_referenced_table": "year_id"
    }},
    {{
      "source_table": "data_files",
      "column_in_source_table": "dataset_id",
      "referenced_table": "datasets",
      "column_in_referenced_table": "dataset_id"
    }},
    {{
      "source_table": "readme_files",
      "column_in_source_table": "dataset_id",
      "referenced_table": "datasets",
      "column_in_referenced_table": "dataset_id"
    }},
    {{
      "source_table": "access_logs",
      "column_in_source_table": "dataset_id",
      "referenced_table": "datasets",
      "column_in_referenced_table": "dataset_id"
    }},
    {{
      "source_table": "access_logs",
      "column_in_source_table": "user_id",
      "referenced_table": "users",
      "column_in_referenced_table": "user_id"
    }}
  ]
}}
[END_DATABASE_SCHEMA]

Now, complete the tasks and generate a database schema containing {table_num} tables based on the following table:
[Markdown Table]
```md
{table}
```
