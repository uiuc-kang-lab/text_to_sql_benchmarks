<h1 align="center">ğŸš€ Alpha-SQL: Zero-Shot Text-to-SQL using Monte Carlo Tree Search</h1>

<div align="center">

[![Homepage](https://img.shields.io/badge/ğŸ -Homepage-blue)](https://alpha-sql-hkust.github.io/)
[![ICML 2025](https://img.shields.io/badge/ICML-2025-FF6B6B.svg)](https://icml.cc/Conferences/2025)
[![arXiv](https://img.shields.io/badge/arXiv-2502.17248-b31b1b.svg)](https://arxiv.org/abs/2502.17248)
[![Slides](https://img.shields.io/badge/ğŸ“Š-Slides-red)](https://liboyan.vip/presentations/Alpha-SQL.pdf)
[![Python](https://img.shields.io/badge/Python-3.11.11-3776AB.svg?style=flat)](https://www.python.org/downloads/release/python-31111/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

</div>

<h4 align="center">âœ¨ If you find our work helpful, please don't hesitate to give us a star â­ !</h4>

<div align="center">
  <img src="assets/intro-figure.png" alt="Introduction Figure" width="600"/>
</div>


## ğŸ“– Introduction
Text-to-SQL, which enables natural language interaction with databases, serves as a pivotal method across diverse industries.
With new, more powerful large language models (LLMs) emerging every few months, fine-tuning has become incredibly costly, labor-intensive, and error-prone. As an alternative, *zero-shot* Text-to-SQL, which leverages the growing knowledge and reasoning capabilities encoded in LLMs without task-specific fine-tuning, presents a promising and more challenging direction.

To address this challenge, we propose **Alpha-SQL**, a novel approach that leverages a Monte Carlo Tree Search (MCTS) framework to iteratively infer SQL construction actions based on partial SQL query states. To enhance the framework's reasoning capabilities, we introduce *LLM-as-Action-Model* to dynamically generate SQL construction *actions* during the MCTS process, steering the search toward more promising SQL queries. Moreover, Alpha-SQL employs a self-supervised reward function to evaluate the quality of candidate SQL queries, ensuring more accurate and efficient query generation.


<div align="center">
  <img src="assets/Alpha-SQL-overview.png" alt="Overview Figure" width="600"/>
</div>

## ğŸ“ Project Structure
```bash
AlphaSQL/
â”œâ”€â”€ ğŸ“‚ data/
â”‚   â””â”€â”€ ğŸ“‚ bird/
â”‚       â””â”€â”€ ğŸ“‚ dev/
â”‚           â”œâ”€â”€ ğŸ“„ dev.json
â”‚           â””â”€â”€ ğŸ“‚ dev_databases/
â”œâ”€â”€ ğŸ“‚ config/
â”‚   â”œâ”€â”€ ğŸ“„ qwen7b_sds_exp.yaml
â”‚   â””â”€â”€ ğŸ“„ qwen32b_bird_dev.yaml
â”œâ”€â”€ ğŸ“‚ results/
â”‚   â””â”€â”€ ğŸ“„ dev_pred_sqls.json
â”œâ”€â”€ ğŸ“‚ script/
â”‚   â”œâ”€â”€ ğŸ“„ preprocess.sh
â”‚   â”œâ”€â”€ ğŸ“„ qwen32b_bird_dev_exp.sh
â”‚   â”œâ”€â”€ ğŸ“„ qwen7b_sds_exp.sh
â”‚   â””â”€â”€ ğŸ“„ sql_selection.sh
â”œâ”€â”€ ğŸ“‚ alphasql/
â”‚   â”œâ”€â”€ ğŸ“‚ runner/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ preprocessor.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ sql_selection.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ mcts_runner.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ selection_runner.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ task.py
â”‚   â”œâ”€â”€ ğŸ“‚ templates/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ schema_selection.txt
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ sql_revision.txt
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ sql_generation.txt
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ raphrase_question.txt
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ identify_column_functions.txt
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ identify_column_values.txt
â”‚   â”‚   â””â”€â”€ ğŸ“„ keywords_extraction.txt
â”‚   â”œâ”€â”€ ğŸ“‚ config/
â”‚   â”‚   â””â”€â”€ ğŸ“„ mcts_config.py
â”‚   â”œâ”€â”€ ğŸ“‚ database/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ sql_execution.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ utils.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ sql_parse.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ schema.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ database_manager.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ lsh_index.py
â”‚   â”œâ”€â”€ ğŸ“‚ llm_call/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ cost_recoder.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ openai_llm.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ prompt_factory.py
â”‚   â””â”€â”€ ğŸ“‚ algorithm/
â”‚       â”œâ”€â”€ ğŸ“‚ selection/
â”‚       â”‚   â””â”€â”€ ğŸ“„ utils.py
â”‚       â””â”€â”€ ğŸ“‚ mcts/
â”‚           â”œâ”€â”€ ğŸ“„ mcts_node.py
â”‚           â”œâ”€â”€ ğŸ“„ mcts_action.py
â”‚           â”œâ”€â”€ ğŸ“„ mcts.py
â”‚           â””â”€â”€ ğŸ“„ reward.py
â”œâ”€â”€ ğŸ“„ README.md
â”œâ”€â”€ ğŸ“„ requirements.txt
â””â”€â”€ ğŸ“„ .env
```

## ğŸ“¥ Dataset Preparation

1. Download required resources:
   - Bird dataset: [Bird Official Website](https://bird-bench.github.io/)

2. Unzip the dataset to `data/bird` directoty following the project structure above.


## ğŸ› ï¸ Environment Setup

1. AlphaSQL Env
    ```bash
    conda create -n alphasql python=3.11
    conda activate alphasql

    pip install -r requirements.txt
    ```

2. VLLM Env
    ```bash
    conda create -n vllm python=3.12 -y
    conda activate vllm

    git clone https://github.com/vllm-project/vllm.git
    cd vllm
    pip install -e .
    ```

## ğŸš€ Deploy Local LLM Using VLLM
```bash
conda activate vllm

# For 4 GPUs
CUDA_VISIBLE_DEVICES=0,1,2,3 vllm serve Qwen/Qwen2.5-Coder-32B-Instruct --served-model-name Qwen/Qwen2.5-Coder-32B-Instruct --port 9999 -tp 4

# For 8 GPUs
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 vllm serve Qwen/Qwen2.5-Coder-32B-Instruct --served-model-name Qwen/Qwen2.5-Coder-32B-Instruct --port 9999 -tp 8
```

## ğŸƒâ€â™‚ï¸Run AlphaSQL

### 1. Switch AlphaSQL Conda Env
```bash
conda activate alphasql
```

### 2. Dataset Preprocessing

1. Modify `OPENAI_API_KEY` and `OPENAI_BASE_URL` in `.env` file (we need to access `text-embedding-3-large` model of OpenAI in preprocessing stage)
    ```bash
    OPENAI_API_KEY = "your-api-key"
    OPENAI_BASE_URL = "your-custom-endopoint" # If you use non-OPENAI services

2. Run the following:
    ```bash
    bash script/preprocess.sh
    ```

### 3. Generate SQL Candidates

1. Modify `OPENAI_API_KEY` and `OPENAI_BASE_URL` in `.env` file (we need to access `Qwen/Qwen2.5-Coder-32B-Instruct` model of VLLM delopyment)
    ```bash
    OPENAI_API_KEY="EMPTY"
    OPENAI_BASE_URL="http://0.0.0.0:9999/v1"
    ```

2. Run the following:
    ```bash
    bash script/qwen32b_bird_dev_exp.sh
    ```

### 4. Select Final SQL

1. Run the following:
    ```bash
    bash script/sql_selection.sh
    ```

3. The final `pred_sqls.json` will in the project root dir (defined in `script/sql_selection.sh` OUTPUT_PATH variable)

## ğŸ“ Citation
If you find our work useful or inspiring, please kindly cite:
```bibtex
@inproceedings{alpha-sql,
  author       = {Boyan Li and
                  Jiayi Zhang and
                  Ju Fan and
                  Yanwei Xu and
                  Chong Chen and
                  Nan Tang and
                  Yuyu Luo},
  title        = {Alpha-SQL: Zero-Shot Text-to-SQL using Monte Carlo Tree Search},
  booktitle    = {Forty-Second International Conference on Machine Learning, {ICML} 2025,
                  Vancouver, Canada, July 13-19, 2025},
  publisher    = {OpenReview.net},
  year         = {2025}
}
```