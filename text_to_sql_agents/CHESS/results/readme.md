# Results - CHESS: Contextual Harnessing for Efficient SQL Synthesis

This folder contains the results of running the CHESS pipeline on various text-to-SQL synthesis tasks. Each file and directory in this folder stores crucial data that captures the performance, configuration, and intermediate outputs generated by the pipeline. All files and folders are contained in the corresponding `.zip` file for each run.

## Files and Directories

### `-args.json`
- This file contains the **configuration arguments** used to run the CHESS pipeline. It captures all the relevant parameters such as model selection, schema pruning settings, and keyword retrieval strategies that were applied in the run leading to the current results.

### `-statistics.json`
- The `statistics` file provides a **summary of the pipeline’s performance**. It contains:
  - The **counts** of questions categorized as `correct`, `incorrect`, or `error`.
  - The **IDs** of the questions in each category, allowing for a quick assessment of how well the pipeline performed in generating SQL queries.

### `(database_id)_(question_id).json`
- These files are the **execution logs** for each specific question in a database. Each JSON file is named according to the format `database_id_question_id` and contains:
  - **Intermediate outputs** from every stage of the pipeline, including data retrieval, schema selection, and SQL synthesis.
  - These logs serve as a detailed record of the pipeline’s step-by-step process for that particular question, which can be helpful for debugging or understanding the synthesis path.

### `logs/` directory
- The `logs/` directory contains **text files capturing the full conversation** between the system and the large language models (LLMs). These logs include:
  - Queries sent to the LLM.
  - Responses from the LLM.
  - Any revision steps or follow-up prompts to correct or refine the generated SQL queries.

---

## Notes on Reproducibility

- Due to the **nondeterministic nature of large language models (LLMs)**, running the pipeline on the same instance may not produce identical results. For more details on LLM determinism, you can refer to [this discussion](https://community.openai.com/t/a-question-on-determinism/8185/4).
- Model updates, such as those periodically released for models like GPT, can also introduce **variations in the results**. As new versions of models are deployed, slight differences may appear in query generation and synthesis.

All in all, these results should serve as a benchmark to assist future studies in **comparing** and **improving upon the intermediate results** of the CHESS pipeline.
